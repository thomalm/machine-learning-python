{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from IPython.html.widgets import interact\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "The **Data Visualization** and **Techniques for Analyzing Data** parts of the presentations have been written in **Python** and are dispayed in **Jupyter**\n",
    "\n",
    "<img style=\"float:left; width:45%\" src=\"http://morganlinton.com/wp-content/uploads/2015/04/python-programming.png\"/>\n",
    "\n",
    "<img style=\"float:left; width:45%; margin-left:5%\" src=\"http://lambdaops.com/jupyter-environments-odsc2015/pictures/jupyter-logo.png\"/>\n",
    "\n",
    "<br clear=\"all\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "* As big data becomes bigger, and more companies deal with complex data sets with dozens of variables, data visualization will become even more important.\n",
    "* With the rise of data warehouses, delivery of data analysis reports has shifted from print to digital.\n",
    "* Data visualization is an important tool when working with data\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "> Visualization is the process that transforms (abstract) data into interactive graphical representations for the purpose of exploration, confirmation or presentation.\n",
    "\n",
    "#### Why use graphics to visualize data?\n",
    "\n",
    "* Figures are richer; provide more information with less clutter and in less space.\n",
    "* Figures provide the gestalt effect: they give an overview; make structure more visible.\n",
    "* Figures are more accessible, easier to understand, faster to grasp, more comprehensible, more memorable, more fun and less formal.\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=-hJmjoowwGU\">Meditteranian Sea Currents</a>\n",
    "\n",
    "**Anscombe's quartet**\n",
    "\n",
    "* Statistics are often used to make a summary of the data\n",
    "* Anscombe's quartet comprises four datasets that have nearly identical simple statistical properties\n",
    "* Still appear very different when graphed. \n",
    "* Created to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties.\n",
    "\n",
    "<img style=\"float:left; width:75%\" src=\"https://camo.githubusercontent.com/859b0bd8625ba6c8f1bb6706bd985189002560b6/687474703a2f2f74682d6d617965722e64652f626f6368756d323031332f696d672f616e73636f6d62655f7461626c652e6a7067\"/>\n",
    "\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "Source: https://github.com/KartikKannapur/Data-Science-CSCI-E-109/wiki/Anscombe's-Quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example taken from the Seaborn documentation\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "# Show the results of a linear regression within each dataset\n",
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=df,\n",
    "           col_wrap=2, ci=None, palette=\"muted\", size=4,\n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Its more difficult to illustrate these differences using other tools than visualization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Fundamentals\n",
    "\n",
    "#### Data types \n",
    "\n",
    "The purpose of a visualization is to encode data in shapes, colors, positions. There are different rules for different **data types**.\n",
    "\n",
    "* **Quantitative Data**\n",
    "    * Life Expectancy\n",
    "    * Income Per Person (continious, measured)\n",
    "    * Total Population (discrete, counted)\n",
    "* **Categorical Data**\n",
    "    * Nominal\n",
    "        * Geographic region\n",
    "        * Gender\n",
    "    * Ordered\n",
    "        * Dates\n",
    "        * Population bins (0-50 million, 50-100 million...)\n",
    "        * Class difficulity (easy, moderate, hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Encodings\n",
    "\n",
    "Visual encodings are mappings from data to display elements. For a visualization to be effective it must be easialy decoded by the reader. Researchers have looked at which visual encodings that work, and which don't. \n",
    "\n",
    "**The Cleveland McGill Scale ranks them in the following order**:\n",
    "\n",
    "* Position\n",
    "* Position on identical but nonaligned scales e.g. multiple scatter plots\n",
    "* Length\n",
    "* Angle and slope\n",
    "* Area\n",
    "* Volume, density and color saturation\n",
    "* Color hue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0.6; x = [1, 2, 3, 4, 5]; y = [3, 2, 4, 2, 5]\n",
    "\n",
    "plt.scatter(x, y, s=100, alpha=a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.linspace(0, 1, 100);\n",
    "x = z * np.sin(20 * z); \n",
    "y = z * np.cos(20 * z)\n",
    "s = z * 300 # Size\n",
    "c = z # Color value\n",
    "\n",
    "def plot_3D(elev=20, azim=40, marker = 'o'):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter(x, y, z, c=c, s=s, marker=marker, cmap='rainbow');\n",
    "    ax.view_init(elev, azim)\n",
    "\n",
    "interact(plot_3D, elev=[0, 90], azim=[0, 90], marker = [',', 'D', 'p', 'o', '*']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General rule:** Try to avoid using 3D data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5]; y = [3, 2, 4, 2, 5]\n",
    "\n",
    "plt.bar(x, y, alpha=a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Angle and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sin = np.linspace(0, 10, 30)\n",
    "\n",
    "plt.plot(sin, np.sin(sin), alpha=a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = [1, 1, 1, 1, 1]; s = [10, 100, 500, 1000, 2000]\n",
    "\n",
    "plt.scatter(x, y, s=s, alpha=a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapes = (',', 'D', 'p', 'o', '*')\n",
    "\n",
    "for _s, _x, _y in zip(shapes, x, y):\n",
    "    plt.scatter(_x, _y, marker=_s, s=1000, alpha=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrast and/or color saturation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that color saturation is ranked over color hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette(\"Greys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.palplot(sns.cubehelix_palette(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "sns.palplot(sns.color_palette(flatui))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What data types and visual encodings are used in this visualization?\n",
    "\n",
    "<a href=\"http://www.gapminder.org/tools/bubbles\">Gapminder.org</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart types\n",
    "\n",
    "When most people think about visualizations they think about graphs or diagrams.\n",
    "\n",
    "<img style=\"float:left; width:75%\" src=\"http://www.infographicsblog.com/wp-content/uploads/2011/11/chart-suggestion-infographic.jpg\"/>\n",
    "\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "Lets start by looking at some **basic chart types**.\n",
    "\n",
    "#### Bar charts\n",
    "\n",
    "A bar chart or bar graph is a chart that presents grouped data with rectangular bars with lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cities = ['Philadelphia', 'Huston', 'Chicago', 'Los Angeles', 'New York']\n",
    "y_pos = np.arange(len(cities))\n",
    "population = [1547607, 2160821, 2714856, 3857799, 8336697]\n",
    "\n",
    "plt.barh(y_pos, population, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, cities)\n",
    "plt.xlabel('Total Population')\n",
    "plt.title('Population of Largest U.S. Cities');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line chart\n",
    "\n",
    "A line chart or line graph is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments It is a basic type of chart common in many fields. It is similar to a scatter plot except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time – a time series – thus the line is often drawn chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Array of 30 points from 0 to 10\n",
    "x = np.linspace(0, 10, 30)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Add noise\n",
    "z = y + np.random.normal(size=30) * .2\n",
    "\n",
    "plt.plot(x, y, 'ro-', label='A sine wave')\n",
    "plt.plot(x, z, 'b-', label='Noisy sine')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel(\"X axis\")\n",
    "plt.ylabel(\"Y axis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have few elements, a bar chart may also work well for time-series. If you have many data points, a bar chart is probably better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plots\n",
    "\n",
    "The data is displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. Useful for finding correlation between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some other chart types.\n",
    "\n",
    "#### Heatmap\n",
    "\n",
    "A heat map is a graphical representation of data where the individual values contained in a matrix are represented as colors. Useful for e.g. market basket visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example taken from the Seaborn documentation\n",
    "flights = sns.load_dataset(\"flights\")\n",
    "flights = flights.pivot(\"month\", \"year\", \"passengers\")\n",
    "ax = sns.heatmap(flights, linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs \n",
    "\n",
    "In mathematics and computer science, graph theory is the study of graphs. These are mathematical structures used to model pairwise relations between objects. A graph is made up of vertices and nodes. A graph may be undirected, meaning that there is no distinction between the two vertices associated with each edge, or its edges may be directed from one vertex to another.\n",
    "\n",
    "<a href=\"http://evelinag.com/blog/2015/12-15-star-wars-social-network\">Star Wars Social Networks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps\n",
    "\n",
    "A choropleth map is a thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical variable being displayed on the map, such as population density or per-capita income. The map is made using **http://datamaps.github.io/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8000/', width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multi-Dimensional Datasets\n",
    "\n",
    "Here you see an example of a pair plot or a scatter plot matrix which plot pairwise relationships in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", size=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lie Factor\n",
    "\n",
    "The “Lie Factor” is a value to describe the relationship between the size of effect shown in a graphic and the size of effect shown in the data.\n",
    "\n",
    "## $\\text{lie factor} = \\frac{\\text{Size of the effect shown in the graphics}}{\\text{Size of the effect shown in the data}}$\n",
    "\n",
    "**General rule**: 0.95 < lie-factor < 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "rainfall = [53, 73, 66, 55, 58, 90, 62, 52, 60, 80, 51, 54]\n",
    "y_pos = np.arange(len(months))\n",
    "\n",
    "plt.bar(y_pos, rainfall, align='center', alpha=0.75)\n",
    "plt.xticks(y_pos, months)\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "plt.title('Monthly Average Rainfall')\n",
    "plt.ylim(50, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(y_pos, rainfall, align='center', alpha=0.75)\n",
    "plt.xticks(y_pos, months)\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "plt.title('Monthly Average Rainfall')\n",
    "plt.ylim(0, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world example from **Fox News**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:70; float:left\" src=\"http://cloudfront.mediamatters.org/static/uploader/image/2014/03/31/obamacareenrollment-fncchart.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory vs. explanatory data visualizations\n",
    "\n",
    "When working with data visualization you often encounter the terms **exploratory data analysis** and **explanatory data visualization**. The first could be described as a conversation between you and your data, the latter a conversation between your data and your audience.\n",
    "\n",
    "#### Exploratory data analysis\n",
    "\n",
    "* Often more practical and simple (we are interested in solving problems)\n",
    "* Choosing the right tool\n",
    "\n",
    "##### Titanic: practical example\n",
    "\n",
    "> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. -- Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic/train.csv\")\n",
    "\n",
    "# display the 5 first rows\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some quick cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lowercase all column names\n",
    "titanic.columns = [c.lower() for c in titanic.columns]\n",
    "\n",
    "# Let's keep only the complete records for visualization purposes\n",
    "df = titanic\n",
    "df = df.drop(['cabin'], axis=1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the features\n",
    "\n",
    "To get an overview of the dataset, lets plot all the numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 6)) \n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 0))\n",
    "df.survived.value_counts().plot(kind='bar', alpha=0.5)\n",
    "plt.title(\"Distribution of Survival, (1 = Survived)\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 1))\n",
    "plt.scatter(df.survived, df.age, alpha=0.1)\n",
    "plt.ylabel(\"age\")\n",
    "plt.title(\"Survial by Age,  (1 = Survived)\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 2))\n",
    "df.pclass.value_counts(ascending=True).plot(kind='barh', alpha=0.5)\n",
    "plt.ylabel(\"Pclass\")\n",
    "plt.title(\"Distribution of passenger class\")\n",
    " \n",
    "plt.subplot2grid((2, 3),(1, 0), colspan=2)\n",
    "df.age[df.pclass == 1].plot(kind='kde')    \n",
    "df.age[df.pclass == 2].plot(kind='kde')\n",
    "df.age[df.pclass == 3].plot(kind='kde')\n",
    "plt.xlabel(\"Age\")    \n",
    "plt.title(\"Age Distribution within classes\")\n",
    "plt.legend(('1st Class', '2nd Class','3rd Class'), loc='best')\n",
    "\n",
    "plt.subplot2grid((2, 3), (1, 2))\n",
    "df.embarked.value_counts().plot(kind='bar', alpha=0.5);\n",
    "plt.title(\"Distribution of port of Embarkation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to predict whether or not a passenger will survive or not. We are therefore interested in finding out what information or features can help us determine the probability that a passenger survived or perished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.survived.value_counts().plot(kind='barh', alpha=.5)\n",
    "plt.title(\"Distribution of Survival, (1 = Survived)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us start breaking it down by gender\n",
    "\n",
    "> Women and children first is a historical code of conduct whereby the lives of women and children were to be saved first in a life-threatening situation (typically abandoning ship, when survival resources such as lifeboats were limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom color palette for our plots\n",
    "pal = dict(male=\"#6495ED\", female=\"#FF6699\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "g = sns.barplot(y=\"sex\", x=\"survived\", data=df, palette=pal, )\n",
    "plt.title(\"Distribution of Survival by Gender (Proportional)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger class\n",
    "\n",
    "> The Preston curve is an empirical cross-sectional relationship between life expectancy and real per capita income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"pclass\", y=\"survived\", hue=\"sex\", data=df, size=6, kind=\"bar\", palette=pal)\n",
    "g.set_ylabels(\"Survival Rate\")\n",
    "plt.title(\"Distribution of Survival by Gender and Passenger Class\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age\n",
    "\n",
    "Lets see how each passenger's age affected their chance of survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot(x=\"age\", y=\"survived\", col=\"sex\", hue=\"sex\", data=df, palette=pal, y_jitter=.02, logistic=True)\n",
    "g.set(xlim=(0, 80), ylim=(-.05, 1.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanatory data visualizations\n",
    "\n",
    "* Often more creative and elaborate\n",
    "* This is the kind of visualization you should be finding in presentations, sales reports and news articles.\n",
    "\n",
    "##### Examples\n",
    "\n",
    "* <a href=\"http://www.nytimes.com/interactive/2012/05/17/business/dealbook/how-the-facebook-offering-compares.html\">Facebook IPO</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for analyzing data\n",
    "\n",
    "### Machine learning\n",
    "\n",
    "* Field of study that gives computer the ability to learn without being explicitly programmed.\n",
    "* Machine learning can be seen as a class of models with tuneable parameter such that the model automatically can adjust its ability to adapt to previously seen data.\n",
    "* First defined in 1959 (why has it become some popular lately?)\n",
    "* Gartner's Top 10 Strategic Technology Trends for 2016 (machine learning is mentioned in three of them)\n",
    "* Machine learning is impacting many critical industries, including healthcare, education, finance, robotics, artificial intelligence, astronomy, and more. \n",
    "* The ability to develop machines and systems that automatically improve, puts machine learning at the absolute forefront of virtually any field that relies on data.\n",
    "\n",
    "> A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -- Tom Mitchell\n",
    "\n",
    "#### AlphaGO\n",
    "\n",
    "* First computer program to defeat a top professional Go player in an even match.\n",
    "* AlphaGo's beat Lee Se-dol 4-1\n",
    "* There is significant strategy involved in the game\n",
    "* The number of possible games is vast ($10^{761}$ compared, $10^{120}$ in chess)\n",
    "* AlphaGo's algorithm uses a combination of machine learning and tree search techniques, combined with extensive training, both from human and computer play.\n",
    "\n",
    "<img style=\"float:left\" src=\"https://gogameguru.com/i/2016/03/AlphaGo-Lee-Sedol-game-3-game-over.jpg\"/>\n",
    "\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "> \"We're very pleased that AlphaGo played some quite surprising and beautiful moves, according to the commentators, which was amazing to see.\"\n",
    "\n",
    "### What a deep networks thinks about your #selfie\n",
    "\n",
    "### Basic Machine Learning concepts\n",
    "\n",
    "* I'll try to do some live code samples where i implement som different models while i try to explain what is happening.\n",
    "* I'm using the **scikit-learn** package in Python to implement the models\n",
    "* **Numpy** and **Pandas** (for handling data) and **matplotlib** for visualizations.\n",
    "\n",
    "#### Supervised Learning: Regression and classification\n",
    "\n",
    "In Supervised Learning, we have a dataset consisting of both features and labels. The task is to construct an estimator which is able to predict the label of an object given the set of features. A simple task would be to make a model that can recognize handwritten digits (I'm showing this later).\n",
    "\n",
    "* **Regression**: predict continious varibles\n",
    "* **Classification**: predict a distinct class\n",
    "\n",
    "**More complicated use cases**\n",
    "\n",
    "- Predicting future results (Regression, Business Forecasting)\n",
    "- Given a photograph of a person, identify the person in the photo (Classification, Facebook)\n",
    "- Based on your previously listened songs, create a personal playlist for you (Recommender Systems, Spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression\n",
    "\n",
    "One of the simplest examples of regression is fitting a straight line to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(0)\n",
    "X = np.random.random(size=(20, 1))\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(20)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate predictions\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "# Plot Data\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "\n",
    "# Plot Regression line\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also more sophisticated models, which can respond to finer features in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Fit a random forest regression model\n",
    "model = RandomForestRegressor() #LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "#X_fit = np.linspace(-1, 2, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "  \n",
    "  There are many supervised learning algorithms available; here we'll introduce one of the most powerful and interesting methods: Support Vector Machines (SVMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# Generate isotropic Gaussian blobs for clustering\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.55)\n",
    "\n",
    "# Plot the data, the label determines the color\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discriminative classifier attempts to draw a line between the two sets of data. However, it is possible to come up with a large set of lines that separate the clusters successfully. Which of the following lines is the **best** one and **why**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')\n",
    "\n",
    "# Our endpoints\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "\n",
    "# Three lines that successfully separate the data\n",
    "for a, b in [(1.0, 1.1), (0.3, 2.1), (-0.2, 2.8)]:\n",
    "    #Plot the line y = ax + b\n",
    "    plt.plot(xfit, a * xfit + b, 'k--')\n",
    "    \n",
    "# What happens to these points?    \n",
    "\n",
    "plt.plot([2.2], [3.0], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "plt.plot([3.0], [2.4], 'x', color='purple', markeredgewidth=2, markersize=10)\n",
    "\n",
    "# Limit the x-axis\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what line a support vector machine would choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVC classifier\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=150, facecolors='none', zorder=10)\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([xi, yj])\n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "def svm_demo(n=50):    \n",
    "    # Generate isotropic Gaussian blobs for clustering\n",
    "    X, y = make_blobs(n_samples=n, centers=2, random_state=0, cluster_std=0.55)\n",
    "    # Fit the SVC classifier\n",
    "    clf.fit(X, y)\n",
    "    # Plot data\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')\n",
    "    # Plot SVC decision boundary\n",
    "    plot_svc_decision_function(clf);\n",
    "    \n",
    "interact(svm_demo, n=[40, 400]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not all points are equally important, only the ones **supporting** the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "Where SVM gets incredibly exciting is when it is used in conjunction with kernels. For the following example it is not possible to linearly separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "X, y = make_circles(200, factor=.2, noise=.1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can adjust this is to apply a kernel, which is some functional transformation of the input data. For example, one simple model we could use is a **radial basis function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives us the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_3D(elev=30, azim=30):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='rainbow')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[0, 90], azip=(0, 180));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the decision boundry looks like applying the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class problem\n",
    "\n",
    "What then if you have multiple classes. If you have multiple classes one approach is to create a one vs. all set of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_multi_class(kernel='linear', centers=4):\n",
    "    X, y = make_blobs(n_samples=200, centers=centers, random_state=0, cluster_std=0.5)\n",
    "    clf = SVC(kernel=kernel).fit(X, y)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap='rainbow', alpha=0.2)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "interact(plot_multi_class, kernel=['linear', 'poly', 'rbf'], centers=[2, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: Dimensionality reduction and clustering\n",
    "\n",
    "In machine learning, the problem of unsupervised learning is that of trying to find hidden structure in unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "> Clustering is the process of examining a collection of “points,” and grouping\n",
    "the points into “clusters” according to some distance measure.\n",
    "\n",
    "The goal is that points in the same cluster have a small distance from one another, while points in different clusters are at a large distance from one another.\n",
    "\n",
    "**Example use-cases**\n",
    "\n",
    "* Find groups of similar users/customers based on their browsing habits (Marketing, Ads)\n",
    "* Given a mixture of two sound sources, for example a person talking over another and separate the two (Blind source separation problem)\n",
    "\n",
    "K Means is an algorithm for **unsupervised clustering**: that is, finding clusters in data based on the data attributes alone (not the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# Generate clusters\n",
    "X, y = make_blobs(n_samples=300, centers=5, random_state=1, cluster_std=0.90)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Income\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see from the data that we have five different clusters. Lets implement K-Means and see the result when varying the number of predifined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plot_clusters(num_clusters=2):\n",
    "    # KMeans with 4 Clusters\n",
    "    k_means = KMeans(num_clusters)\n",
    "    # Fit the model on the attributes\n",
    "    k_means.fit(X)\n",
    "    # Predict the labels for each point\n",
    "    y_pred = k_means.predict(X)\n",
    "    # Get the cluster centers\n",
    "    mu = k_means.cluster_centers_\n",
    "    # Plot the result\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='rainbow', alpha=0.4);\n",
    "    plt.scatter(mu[:, 0], mu[:, 1], s=200, c=np.unique(y_pred), cmap='rainbow')\n",
    "    print(\"Cluster Centers\")\n",
    "    print(mu)\n",
    "\n",
    "interact(plot_clusters, num_clusters=[1, 10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason: Validation and model selection\n",
    "\n",
    "One of the most important pieces of machine learning is <b>model validation</b>: that is checking how well your model performs or fits a particular dataset. This is often referred to as **offline validation**. **Online validation** refers to online experiments designed to measure the true performance of your model such as e.g. **A/B-testing**.\n",
    "\n",
    "Lets look at a practical example using the famous MNIST handwritten digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = pd.read_csv(\"digits/train.csv\")[:5000]\n",
    "\n",
    "# Extract the feature and label values\n",
    "X = digits.iloc[:,1:].values\n",
    "y = digits[[0]].values.ravel()\n",
    "\n",
    "digits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_digits(img, labels, nrows, ncols, colors=[]):\n",
    "    plt.figure(figsize=(min(16, ncols*2), min(16, nrows*2)))\n",
    "    for i in range(nrows * ncols):\n",
    "        plt.subplot(nrows, ncols, i+1)\n",
    "        # Reshape every image to a square array 2d array\n",
    "        sqrt = np.sqrt(len(img[i]))\n",
    "        digit = img[i].reshape(sqrt, sqrt)\n",
    "        plt.imshow(digit, interpolation='nearest')\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        plt.title(labels[i])\n",
    "\n",
    "plot_digits(X, y, 2, 8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation sets\n",
    "\n",
    "To assess how well our model performs we cannot test our model on the same data that was used for training. <b>This is generally not a good idea</b>. If we optimize our estimator this way, we will tend to over-fit the data.\n",
    "\n",
    "A better way to test a model is to use a hold-out set which doesnt enter the training.\n",
    "\n",
    "#### Hold-Out Dataset\n",
    "\n",
    "Split the dataset into two parts:\n",
    "\n",
    "* **Training set** used to train the classifier\n",
    "* **Validation/Test set** used to estimate the error rate of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Generate a 75:25 hold-out dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_digits(X_test, ['?'] * 8, 1, 8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict labels for our test set\n",
    "\n",
    "We can now use classifier to predict the labels for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the RF model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for our hold-out dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy by checking the results against our hold-out dataset labels\n",
    "print(\"{0} / {1} correct\".format(np.sum(y_test == y_pred), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten our digits so they play nicely with my plot function\n",
    "digits = [x.flatten() for x in X_test]; labels = []\n",
    "\n",
    "# Generate labels with both the predicted and true value\n",
    "for pred, test in zip(y_pred, y_test):\n",
    "    labels.append(str(pred) + \" (\" + str(test) + \")\")\n",
    "\n",
    "plot_digits(digits, labels, 4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix can give us more information about our models performance. More specifically how well our model is at predicting the differnet label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(cm, cmap='RdPu', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generally predicts the correct label. However we can see that the model some times misses on:\n",
    "\n",
    "* 2 and 7\n",
    "* 3 and 8\n",
    "* 4 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold cross-validation\n",
    "\n",
    "One of the main problems with validation sets is that you \"lose\" some of the data. Above we only used 75% of the data for training and 25% for validation. Another option is to use K-fold cross-validation where you split the data into k subsets and repeat the hold-out method k times, where each chunk gets a turn as the validation set. Then the average error across all k trials is computed.\n",
    "\n",
    "<img style=\"float:left\" src=\"https://qph.is.quoracdn.net/main-qimg-c46f088d0ebf6598226e22aeac930512?convert_to_webp=true\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#Perform 10-fold cross validation\n",
    "cv = cross_val_score(RandomForestClassifier(n_estimators=100), X, y, cv=10)\n",
    "\n",
    "print(\"Cross Validation Scores\")\n",
    "print(cv)\n",
    "print()\n",
    "\n",
    "print(\"Mean Score\")\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us an even better idea of how well our model is doing.\n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "The **evaluation metric** chosen depends on what type of problem you are trying to solve. Are you predicting housing prices or how well a playlist on spotify is you need to use other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, underfitting and model selection\n",
    "\n",
    "One of the most important pieces of machine learning is model validation: that is, checking how well your model fits a given dataset. But there are some pitfalls you need to watch out for.\n",
    "\n",
    "#### Illusatration of bias-variance trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_func(x, err=0.5):\n",
    "    y = 10 - (1. / (x + 0.1))\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y\n",
    "\n",
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    # randomly sample the data\n",
    "    np.random.seed(1)\n",
    "    X = np.random.random(N)[:, np.newaxis]\n",
    "    y = test_func(X.ravel(), error)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40, error=0.5)\n",
    "\n",
    "#Plot the data\n",
    "plt.scatter(X.ravel(), y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model attempts to find a straight-line through the data\n",
    "* Because the data is more complicated than a straight line, the model cannot describe the dataset well\n",
    "* Such a model is said to **underfit** the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression**\n",
    "\n",
    "$y = ax^2 + by + c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we give our model more flexibility it can describe the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_regression(r=20, plot_y=False):\n",
    "    model = PolynomialRegression(r)\n",
    "    model.fit(X, y)\n",
    "    y_test = model.predict(X_test)\n",
    "    plt.scatter(X.ravel(), y)\n",
    "    plt.plot(X_test.ravel(), y_test)\n",
    "    plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "    plt.ylim(-4, 14)\n",
    "    plt.plot(X_test.ravel(), y_test);\n",
    "    if plot_y:\n",
    "        plot_true_y()\n",
    "    \n",
    "def plot_true_y():\n",
    "    x = np.arange(0, 1, 0.02);\n",
    "    # The real y function\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    plt.plot(x, y, '--')\n",
    "\n",
    "interact(plot_regression, r=[1, 100], plot_y=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when fitting a higher-order polynomial through the data.\n",
    "\n",
    "* The model fit has enough flexibility for almost perfectly fitting the data\n",
    "* However, the model learns the **data** to well rather than the intrinsic properties of the process that generated the data\n",
    "* Such a model is said to **overfit** the data, meaning that it also ends up accounting for random errors in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left; width:50%\" src=\"http://zhangjunhd.github.io/assets/2014-10-01-bias-variance-tradeoff/1.png\"/>\n",
    "\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "Source: http://zhangjunhd.github.io/2014/10/01/bias-variance-tradeoff.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
